去重的操作：
	已爬队列，当前这个页面处理时需要在已爬队列中
做一次去重；
	待爬队列，为了保证队列里url的唯一性，这里需要
在待爬队列中去重；

    # 队列的方式来模拟广度优先遍历
    crawl_queue = []   #待爬队列
    crawled_queue = [] #已爬队列
    for itemUrl in itemUrls:
        # 在已爬队列中做去重处理
        if itemUrl not in crawled_queue:
            # 在已爬队列中没有的话，则插入待爬队列中
            crawl_queue.append(itemUrl)
    # 在待爬队列中再做一次去重
    crawl_queue = list(set(crawl_queue))
        
    # 抓取队列中的信息为空，则退出循环
    while crawl_queue:
        url = crawl_queue.pop(0)
        # 用进程池中的进程来处理这个URL
        pool.apply_async(func=CrawlInfo, args=(url, q))
        
        # 处理完之后，需要把这个url放入已爬队列中
        url = q.get()
        crawled_queue.append(url)
说明：
1）待爬队列则既需要append，也需要pop；
已爬队列中只有append操作；
2）爬虫开始于向待爬队列中append一个seedUrl；
结束于待爬队列为空时；
3）在做去重的比较时，可以比较HASH之后的结果；
如果数据量巨大的话，可以使用多次HASH来避免碰撞；
3次HASH；
     
在生产环境中，
	一般使用Redis数据库来做去重操作；
Redis是一种内存数据库，URL，URL哈希值，及
抓取到的数据等保存在其中；
在数据量大的时候，使用这种数据库的解决方案效率会
高很多；缺点是还有硬件的限制，数据还是需要考虑
在内存中的时效性和安全性；

安装Scrapy：
	conda install scrapy
制作一个Scrapy爬虫项目的步骤：
1.创建项目：
	scrapy startproject tencentSpider
2.进到下一层目录：
    cd 	tencentSpider
3.创建爬虫:
	scrapy genspider tencent hr.tencent.com
生成一个具体的爬虫
	scrapy crawl tencent
要让我们的爬虫程序能够正式的抓取数据，
我们需要修改：
            setttings.py 设置
				1）注释掉Robots协议；
				2）打开Headers选项，添加UA；
				3）打开PIPELINES，准备保存数据；
			items.py     保存item的映射
			    添加将来要抓取数据的信息：
				positionName = scrapy.Field() #名称
				positionLink = scrapy.Field() #详情链接
				positionType = scrapy.Field() #类别
			pipelines.py 保存的逻辑
				完善process_item保存数据的方法；
			tecent.py,抓取页面信息和继续跳转的逻辑
				

	
	
	
面试题：
	百度公司有3万名员工，
请按照员工的年龄给员工排序。
    ages = {}
	ages[i]:
	   i: 14-100, value -> 0
	25 -> ages[25] += 1
	28 -> ages[28] += 1
	...
	O(N)
思考题：有个论坛系统，这个论坛系统有个“灌水王”。
他发的帖子超过了半数，请设计一个算法，
快速的将其ID找出。
	











