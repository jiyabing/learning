机器学习
一、人工智能、机器学习与深度学习
人工智能
       机器学习
              经典机器学习
              基于神经网络的机器学习
                     浅层学习
                     深层学习(深度学习)
              强化学习
              迁移学习
2->   ->4
3->   ->6
2.1->   ->4.2
1.9->   -> ? 3.8
二、机器学习基本类型
1.有监督学习：根据已知的输入和输出，建立联系它们的模型，根据该模型对未知输出的输入进行判断。
1)回归：以无限连续域的形式表示输出。
2)分类：以有限离散域的形式表示输出。
2.无监督学习：在一组没有已知输出(标签)的输入中，根据数据的内部特征和联系，找到某种规则，进行族群的划分――聚类。
3.半监督学习：从一个相对有限的已知结构中利用有监督学习的方法，构建基本模型，通过对未知输入和已知输入的比对，判断其输出，扩展原有的已知领域。
三、机器学习的基本过程
数据采集->数据清洗->数据预处理->选择模型->训练模型
原材料        去除杂质    准备               算法           规则
                                                                             |
                                                                             v
                                                           使用模型<-测试模型
                                                           业务生产    检验
四、数据预处理
              一列一特征
                        |
                        v
一行一样本 -> x x x x x \                  y y y
                        x x x x x  | 样本矩阵  y y y
                        x x x x x /                  y y y
姓名    年龄    身高    体重    ...
张飞    22       1.75   60      
赵云    20       1.80   70
...
1.均值移除
为了统一样本矩阵中不同特征的基准值和分散度，可以将各个特征的平均值调整为0，标准差调整为1，这个过程称为均值移除。
a b c
m=(a+b+c)/3
a-m b-m c-m
m'=(a-m+b-m+c-m)/3=(a+b+c)/3-3m/3=0
A B C
s=sqrt((A^2+B^2+C^2)/3)
A/s B/s C/s
s'=sqrt((A^2/s^2+B^2/s^2+C^2/s^2)/3)
   =sqrt((A^2+B^2+C^2)/s^2/3)
   =sqrt(s^2/s^2)
   =1
sklearn.preprocessing.scale(原始样本矩阵)
    ->均值移除后的样本矩阵
代码：std.py
2.范围缩放
统一样本矩阵中不同特征的最大值和最小值范围。
k x + b = y
k min + b = min'
k max + b = max'
sklearn.preprocessing.MinMaxScaler(
     feature_range=期望最小和最大值)->范围缩放器
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化：为了用占比表示特征，用每个样本的特征值除以该样本的特征值绝对值之和，以使每个样本的特征值绝对值之和为1
          Python Java C/C++ PHP
2016 30         50     40        20    30/140
2017 20         30     20        10    20/80
sklearn.preprocessing.normalize(原始样本矩阵,
   norm='l1')->归一化后的样本矩阵
l1即L1范数，矢量中各元素绝对值之和。
代码：nor.py
4.二值化：用0和1来表示样本矩阵中相对于某个给定阈值高于或者低于它的元素。
sklearn.preprocessing.Binarizer(threshold=阈值)
    ->二值化器
二值化器.transform(原始样本矩阵)->二值化后的样本矩阵
代码：bin.py
5.独热编码
1        3        2
7        5        4
1        8        6
7        3        9
1:10  3:100 2:1000
7:01  5:010 4: 0100
          8:001 6: 0010
                     9: 0001
1 0 1 0 0 1 0 0 0
0 1 0 1 0 0 1 0 0
1 0 0 0 1 0 0 1 0
0 1 1 0 0 0 0 0 1
sklearn.preprocessing.OneHotEncoder(
    sparse=是否使用压缩格式, dtype=元素类型)
    ->独热编码器
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵，同时构建编码表字典
独热编码器.transform(原始样本矩阵)
    ->独热编码后的样本矩阵，使用已有编码表字典
代码：ohe.py
6.标签编码：将字符形式的特征值映射为整数。
sklearn.preprocessing.LabelEncoder()->标签编码器
标签编码器.fit_transform(原始样本矩阵)
    ->编码样本矩阵，构建编码字典
标签编码器.transform(原始样本矩阵)
    ->编码样本矩阵，使用编码字典
标签编码器.inverse_transform(编码样本矩)
    ->原始样本矩阵，使用编码字典
代码：lab.py
五、线性回归
m个输入样本 -> m个输出标签
                 x1 -> y1
                 x2 -> y2
                 x3 -> y3
                 ...
                 xm -> ym
           xk + b -> y
1.预测函数：联系输出和输入的数学函数。
y=kx+b
其中的k和b称为模型参数，根据已知输入样本和对应的输出标签来训练得出。
2.均方误差：每一个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值。
kx1+b=y1'
kx2+b=y2'
kx3+b=y3'
...
kxm+b=ym'
(y1-y1')^2+(y2-y2')^2+(y3-y3')^2+...+(ym-ym')^2
-------------------------------------------------------------
                                         m
3.成本函数：将均方误差看作是关于模型参数的函数，谓之成本函数，记做J(k,b)。
线性回归问题的本质就是寻找能够使成本函数J(k,b)极小值的模型参数。
4.梯度下降
loss = J(k, b)
5.接口
sklearn.linear_model.LinearRegression()->线性回归器
线性回归器.fit(输入样本, 输出标签)
线性回归器.predict(输入样本)->预测输出标签
6.复用
通过pickle将内存中的模型对象写入磁盘文件，或从磁盘文件载入内存，以此保存训练好的模型，以备复用。
代码：line.py、save.py、load.py
六、岭回归
loss = J(k, b)+正则函数(样本权重)x正则强度
                                                          惩罚系数

















































