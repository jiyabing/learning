机器学习
一、人工智能、机器学习与深度学习
人工智能
       机器学习
              经典机器学习
              基于神经网络的机器学习
                     浅层学习
                     深层学习(深度学习)
              强化学习
              迁移学习
2->   ->4
3->   ->6
2.1->   ->4.2
1.9->   -> ? 3.8
二、机器学习基本类型
1.有监督学习：根据已知的输入和输出，建立联系它们的模型，根据该模型对未知输出的输入进行判断。
1)回归：以无限连续域的形式表示输出。
2)分类：以有限离散域的形式表示输出。
2.无监督学习：在一组没有已知输出(标签)的输入中，根据数据的内部特征和联系，找到某种规则，进行族群的划分――聚类。
3.半监督学习：从一个相对有限的已知结构中利用有监督学习的方法，构建基本模型，通过对未知输入和已知输入的比对，判断其输出，扩展原有的已知领域。
三、机器学习的基本过程
数据采集->数据清洗->数据预处理->选择模型->训练模型
原材料        去除杂质    准备               算法           规则
                                                                             |
                                                                             v
                                                           使用模型<-测试模型
                                                           业务生产    检验
四、数据预处理
              一列一特征
                        |
                        v
一行一样本 -> x x x x x \                  y y y
                        x x x x x  | 样本矩阵  y y y
                        x x x x x /                  y y y
姓名    年龄    身高    体重    ...
张飞    22       1.75   60      
赵云    20       1.80   70
...
1.均值移除
为了统一样本矩阵中不同特征的基准值和分散度，可以将各个特征的平均值调整为0，标准差调整为1，这个过程称为均值移除。
a b c
m=(a+b+c)/3
a-m b-m c-m
m'=(a-m+b-m+c-m)/3=(a+b+c)/3-3m/3=0
A B C
s=sqrt((A^2+B^2+C^2)/3)
A/s B/s C/s
s'=sqrt((A^2/s^2+B^2/s^2+C^2/s^2)/3)
   =sqrt((A^2+B^2+C^2)/s^2/3)
   =sqrt(s^2/s^2)
   =1
sklearn.preprocessing.scale(原始样本矩阵)
    ->均值移除后的样本矩阵
代码：std.py
2.范围缩放
统一样本矩阵中不同特征的最大值和最小值范围。
k x + b = y
k min + b = min'
k max + b = max'
sklearn.preprocessing.MinMaxScaler(
     feature_range=期望最小和最大值)->范围缩放器
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化：为了用占比表示特征，用每个样本的特征值除以该样本的特征值绝对值之和，以使每个样本的特征值绝对值之和为1
          Python Java C/C++ PHP
2016 30         50     40        20    30/140
2017 20         30     20        10    20/80
sklearn.preprocessing.normalize(原始样本矩阵,
   norm='l1')->归一化后的样本矩阵
l1即L1范数，矢量中各元素绝对值之和。
代码：nor.py
4.二值化：用0和1来表示样本矩阵中相对于某个给定阈值高于或者低于它的元素。
sklearn.preprocessing.Binarizer(threshold=阈值)
    ->二值化器
二值化器.transform(原始样本矩阵)->二值化后的样本矩阵
代码：bin.py
5.独热编码
1        3        2
7        5        4
1        8        6
7        3        9
1:10  3:100 2:1000
7:01  5:010 4: 0100
          8:001 6: 0010
                     9: 0001
1 0 1 0 0 1 0 0 0
0 1 0 1 0 0 1 0 0
1 0 0 0 1 0 0 1 0
0 1 1 0 0 0 0 0 1
sklearn.preprocessing.OneHotEncoder(
    sparse=是否使用压缩格式, dtype=元素类型)
    ->独热编码器
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵，同时构建编码表字典
独热编码器.transform(原始样本矩阵)
    ->独热编码后的样本矩阵，使用已有编码表字典
代码：ohe.py
6.标签编码：将字符形式的特征值映射为整数。
sklearn.preprocessing.LabelEncoder()->标签编码器
标签编码器.fit_transform(原始样本矩阵)
    ->编码样本矩阵，构建编码字典
标签编码器.transform(原始样本矩阵)
    ->编码样本矩阵，使用编码字典
标签编码器.inverse_transform(编码样本矩)
    ->原始样本矩阵，使用编码字典
代码：lab.py
五、线性回归
m个输入样本 -> m个输出标签
                 x1 -> y1
                 x2 -> y2
                 x3 -> y3
                 ...
                 xm -> ym
           xk + b -> y
1.预测函数：联系输出和输入的数学函数。
y=kx+b
其中的k和b称为模型参数，根据已知输入样本和对应的输出标签来训练得出。
2.均方误差：每一个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值。
kx1+b=y1'
kx2+b=y2'
kx3+b=y3'
...
kxm+b=ym'
(y1-y1')^2+(y2-y2')^2+(y3-y3')^2+...+(ym-ym')^2
-------------------------------------------------------------
                                         m
3.成本函数：将均方误差看作是关于模型参数的函数，谓之成本函数，记做J(k,b)。
线性回归问题的本质就是寻找能够使成本函数J(k,b)极小值的模型参数。
4.梯度下降
loss = J(k, b)
5.接口
sklearn.linear_model.LinearRegression()->线性回归器
线性回归器.fit(输入样本, 输出标签)
线性回归器.predict(输入样本)->预测输出标签
6.复用
通过pickle将内存中的模型对象写入磁盘文件，或从磁盘文件载入内存，以此保存训练好的模型，以备复用。
代码：line.py、save.py、load.py
六、岭回归
loss = J(k, b)+正则函数(样本权重)x正则强度
                                                          惩罚系数
sklearn.linear_model.Ridge(正则强度,
    fit_intercept=是否修正截距,
    max_iter=最大迭代次数)->岭回归器
代码：rdg.py
七、欠拟合与过拟合
欠拟合：无论是训练数据还是测试数据，模型给出的预测值和真实值都存在较大的误差。
过拟合：模型对于训练数据具有较高的精度，但对测试数据则表现极差。模型过于特殊，不够一般（泛化）。
欠拟合<--模型复杂度-->过拟合
八、多项式回归
x -> y                  y = kx + b
x x^2 -> y          y = k1x^2 + k2x + b
x x^2 x^3 -> y  y = k1x^3 + k2x^2 + k3x + b
sklearn.preprocessing.PolynomialFeatures(最高次数)
->多项式特征扩展器
sklearn.pipeline.make_pipe(多项式特征扩展器,
    线性回归器)->多项式回归器
 x-->多项式特征扩展器--x,x^2,x^3...-->线性回归器
                                                                        -->k1,k2,k3-->
代码：poly.py
九、决策树
相似的输入会有相似的输出。
0 - 专科    0-普通     0-女   0-差               0-低
1 - 本科    1-985     1-男   1-及格            1-中
2 - 硕士    2-211               2-良好            2-高
3 - 博士                             3-优异
      学历       院校     性别      成绩      ->      月薪
         1            0         1           2                  8000
         0            0         0           2                  7000
         3            1         1           3                  20000
         ...
         1            1         0           1                  ?
回归-平均
分类-投票
优化：
1)结合业务优先选择有限的主要特征，划分子表，降低决策树的高度；
2)根据香农定理计算根据每一个特征划分子表前后的信息熵差，选择熵减少量最大的特征，优先参与子表划分；
3)集合算法：根据不同方法，构建多个决策树，利用它们的预测结果，按照取均（回归）或投票（分类）的方法，产生最终的预测结果。
A.自助聚合：采用有放回抽样的规则，从m个样本中随机抽取n个样本，构建一棵决策树，重复以上过程b次，得到b棵决策树。利用每棵决策树的预测结果，根据平均或者投票得到最终预测结果。
B.随机森林：在自助聚合算法的基础上更进一步，对特征也应用自助聚合，即每次训练时，不是用所有的特征构建树结构，而是随机选择部分特征参与构建，以此避免特殊特征对预测结果的影响。
C.正向激励：初始化时，针对m个样本分配初始权重，然后根据这个带有权重的模型预测训练样本，针对那些预测错误的样本，提高其权重，再构建一棵决策树模型，重复以上过程，得到b棵决策树。。。
sklearn.tree.DecisionTreeRegressor()->决策树回归器
sklearn.ensemble.AdaBoostRegressor(元回归器,
    n_estimators=评估器数, radom_state=随机种子源)->正向激励回归器
sklearn.ensemble.RandomForestRegressor(
    max_depth=最大树高, n_estimators=评估器数,
    min_samples_split=划分子表的最小样本数)->随机森林回归器
代码：house.py
决策树模型.feature_importances_: 特征重要性
代码：fi.py、bike.py
三、简单分类器
输入    输出
3   1      0
2   5      1
1   8      1
6   4      0
5   2      0
3   5      1
4   7      1
4  -1     0
7   5      ?->0
代码：simple.py
四、逻辑分类
1.预测函数
x1 x2 -> y
              1
y = -----------
       1 + e^-z
z = k1x1 + k2x2 + b
2.成本函数
交叉熵误差
J(k1,k2,b) = sigma(-ylog(y')-(1-y)log(1-y'))/m +
                      m
                      正则函数(||k1,k2,b||)x正则强度
x x -> 0.9 1
x x -> 0.2 0
sklearn.linear_model.LogisticRegression(
    solver='liblinear', C=正则强度)
                  A   B    C
... -> A 1 0.9 0.1 0.3 A
... -> B 0 0.3 0.6 0.4 B
... -> C 0 0.1 0.2 0.6 C
代码：mlog.c
五、朴素贝叶斯分类
x x ... x  -> 0
x x ... x  -> 1
x x ... x  -> 0
x x ... x  -> 0
x x ... x  -> 1
x x ... x  -> 2
x x ... x  -> 1
x x ... x  -> 0
x x ... x  -> 2
...
1 9 ... x  -> 0 0.8
              -> 1 0.9 *
              -> 2 0.7
1.贝叶斯定理
                P(A)P(B|A)
P(A|B) = -------------
                     P(B)
2.朴素贝叶斯分类
求X样本属于C类别的概率，即当观察到X样本出现时，其所属的类别为C的概率：
P(C|X)=P(C)P(X|C)/P(X)
P(C)P(X|C)=P(C,X)=P(C,x1,x2,...,xn)
                                =P(x1,x2,...,xn,C)
                                =P(x1|x2,...,xn,C)P(x2,x3,...,xn,C)
=P(x1|x2,...,xn,C)P(x2|x3,...,xn,C)P(x3,x4,...,xn,C)
=P(x1|x2,...,xn,C)P(x2|x3,...,xn,C)P(x3|x4,...,xn,C)...P(C)
朴素：条件独立假设，即样本各个特征之间并无关联，不构成条件约束。
=P(x1|C)P(x2|C)P(x3|C)...P(C)
X样本属于C类别的概率，正比于C类别出现的的概率乘以C类别条件下X样本中每一个特征值出现的概率之乘积。
代码：nb.py
3.划分训练集和测试集
sklearn.model_selection.train_test_split(
    输入集合, 输出集合, test_size=测试集占比,
    random_state=随机种子源)
    ->训练输入, 测试输入, 训练输出, 测试输出
代码：split.py
4.交叉验证
1)查准率和召回率
查准率：
被正确识别为某类别的样本数/被识别为该类别的样本数
正确性，对不对
召回率：
被正确识别为某类别的样本数/该类别的实际样本数
完整性，全不全
f1_score=2x查准率x召回率/(查准率+召回率)
0<-->1
差     好
sklearn.model_selection.cross_val_score(
    分类器, 输入集合, 输出集合, cv=验证次数,
    scoring=验证指标名称)->验证指标值数组
ms.cross_val_score(model, x, y, cv=5,
    scoring='f1_weighted')->[0.6 0.8 0.4 0.7 0.6]
代码：cv.py
5.混淆矩阵
以实际类别为行，以预测类别为列。
      0     1    2
0  45    4    3
1  11  56    2
2    5    6  49
sklearn.metrics.confusion_matrix(
    实际输出, 预测输出)->混淆矩阵
代码：cm.py
6.分类报告
sklearn.metrics.classification_report(
    实际输出, 预测输出)->分类报告
代码：cr.py
六、随机森林分类
1.评估汽车档次
代码：car.py
2.验证曲线
f1_score = f(模型对象超参数)
验证曲线的峰值，寻找相对理想的超参数。
model = se.RandomForestClassifier(
    max_depth=8, n_estimators=200, random_state=7)
                              ^^^^^^^^^^^^
model = se.RandomForestClassifier(
    max_depth=8, random_state=7)
sklearn.model_selection.validation_curve(
    model, x, y, 'n_estimators', [100, 200, 300, ...],
    cv=5)->训练集得分矩阵, 测试集得分矩阵
             1    2    3    4    5
100 -> 0.7 0.9 0.6 0.8 0.7
200 ->
300 ->
...
代码：vc.py
3.学习曲线
f1_score = f(训练集大小)
sklearn.model_selection.learning_curve(
    model, x, y, 训练集大小数组, cv=5)->
    训练集大小数组, 训练集得分矩阵, 测试集得分矩阵
代码：lc.py
七、支持向量机(SVM)
1.分类边界
同时满足四个条件：
A.正确分类
B.支持向量到分类边界的距离相等
C.间距最大
D.线性（直线，平面）
2.升维变换
对于在低维度空间中无法线性划分的样本，通过升维变换，在高纬度空间寻找最佳线性分类边界。
核函数：用于对特征值进行升维变换的函数。
多项式核函数
径向基核函数
代码：svm_line.py、svm_poly.py、svm_rbf.py
3.当不同类别的样本数量相差悬殊时，样本数较少的类别可能被支持向量机分类器忽略，为此可以将class_weight参数指定为balanced，通过调节不同类别样本的权重均衡化。
代码：svm_balanced.py
4.置信概率
svm.SVC(..., probability=True, ...)
支持向量机分类器.predict_proba(输入样本)->置信概率矩阵
                类别1  类别2
样本1 ->  0.99    0.01
样本2 ->  0.02    0.98
...
代码：svm_prob.py
5.最优超参数
sklearn.model_selection.GridSearchCV(
    模型, 参数组合表, cv=交叉验证次数)->最优模型对象
参数组合表：[{参数名: [取值列表]}, {}, ...]
代码：bhp.py
事件预测
代码：evt.py
2   4
3   6
4   8
...
y = kx + b
利用支持向量机回归模型预测交通流量
代码：trf.py
八、聚类
1.K均值
根据事先给定的聚类数，为每个聚类随机分配中心点，计算所有样本与各个中心点的距离，将每个样本分配到与其距离最近的中心点所在的聚类中。计算每个聚类的几何中心，用该几何中心作为新的聚类中心，重新划分聚类。直到计算出的几何中心与上一次聚类使用的聚类中心重合或者足够接近为止。
聚类数必须事先已知：从业务中找，选择最优化指标。
聚类结果会受样本比例的影响。
聚类中心的初始位置会影响聚类结果。
代码：km.py
图像量化
代码：quant.py
2.均值漂移
把训练样本看成服从某种概率密度函数规则的随机分布，在不断迭代的过程中试图寻找最佳的模式匹配，该密度函数的峰值点就是聚类的中心，为该密度函数所覆盖的样本即隶属于该聚类。
不需要实现给定聚类数，算法本身具有发现聚类数量的能力。
代码：shift.py
3.凝聚层次
凝聚层次聚类，可以是自下而上(聚)，也可以是自上而下(分)的。在自下而上的算法中，每个训练样本都被看做是一个单独的集群，根据样本之间的相似度，将其不断合并，直到集群数达到事先指定的聚类数为止。在自上而下的算法中，所有训练样本被看做是一个大的聚类，根据样本之间的差异度，将其不断拆分，直到集群数达到指定的聚类数为止。
代码：agglo.py
凝聚层次算法，不同于其它基于中心的聚类算法，用它对一些在空间上具有明显连续性，但彼此间的距离未必最近的样本，可以优先聚集，这样所构成的聚类划分就能够表现出较强的连续特性。
代码：spiral.py
4.DBSCAN
"朋友的朋友也是朋友"
从任何一个训练样本出发，以一个事先给定的半径做圆，凡是不在此圆之外的样本都与圆心样本同类，再以这些同类样本为中心做圆重复以上过程，直到没有新的同类样本加入该聚类为止。以此类推，获得样本空间中的所有聚类。那些不属于任何聚类的样本，被称为偏离样本，位于聚类边缘的样本，则成为外周样本，其余统一称为核心样本。
代码：dbscan.py
5.轮廓系数
表示聚类划分内密外疏的程度。
轮廓系数有一下两个指标构成：
a: 一个样本与其所在聚类其它样本的平均距离。
b: 一个样本与其距离最近的另一个聚类中样本的平均距离。
针对这个一个样本的轮廓系数：
s=(b-a)/max(a, b)
针对一个数据集，其轮廓系数就是其中所有样本的轮廓系数的平均值。轮廓系数的值介于[-1, 1]区间，1表示完美聚类，-1表示错误聚类，0表示聚类重叠。
代码：score.py
九、推荐引擎
1.管线
-输入->学习模型1-输出->学习模型2-输出->...
代码：map.py、reduce.py

                          输入
                             |
                            v
def 学习模型1 (参数):
    ...
    return 返回值
                     |
                     v
                  输出--+
                             |
                            v
def 学习模型2 (参数):
    ...
    return 返回值
                     |
                    v
                  输出
所谓管线，其本质就是函数的级联调用，即用一个函数的返回值作为另一个函数的参数。
代码：cc1.py、cc2.py
特征选择器-随机森林分类器
代码：pipe.py
2.寻找最近邻(FNN)
sklearn.neighbors.NearestNeighbors(
    n_neighbors=邻居数, algorithm=算法)->FNN模型
                                                             'ball_tree'
FNN模型.fit(已知样本集合)
x x ... x
x x ... x
...
x x ... x  11
...
x x ... x  23
...
x x ... x  34
...
FNN模型.kneibhbors(待求样本集合)
    ->距离矩阵, 近邻下标索引矩阵
x x ... x   11 23 34   0.5 0.3 0.1
x x ... x   22 10 15   0.4 0.1 0.2
...            ...               ...
代码：fnn.py
3.KNN分类和回归
遍历训练集中的所有样本，计算每个样本与待测样本的距离，并从中挑选出K的最近邻。根据与距离成反比的权重，做加权投票
（分类）或平均（回归），得到待测样本的类别标签或预测数值。
代码：knnc.py、knnr.py
4.欧氏(欧几里得)距离
(x1,y1) <----> (x2,y2)
  __________________________
V (x1-x2)^2 + (y1-y2)^2
  ________________________________________
V (x1-x2)^2 + (y1-y2)^2 + (z1-z2)^2
(a, b, c, ...) <----> (A, B, C, ...)
                                  1
欧氏距离得分=---------------
                        1 + 欧氏距离
0 <-不相似-欧氏距离得分-相似-> 1
代码：es.py
           用户1   用户2   用户3   ...
用户1         1      0.8       0.9   ...
用户2      0.8         1       0.7   ...
用户3
...
5.皮(尔逊)氏距离得分
用两个样本的协方差([-1, 1])表示相似度。
    A    B    C
1  5    1     3
2  10  0     5
代码：ps.py
根据样本的相似程度排序
代码：sim.py
生成针对每个用户的推荐列表
代码：rcm.py
十、文本分析
import nltk - 自然语言工具包
1.分词
从完整的文章或段落中，划分出若干独立的语义单元，如句或者词。
代码：tkn.py
2.词干提取
从单词中抽取主要成分，未必是合法的词汇。
代码：stm.py
3.词型还原
从名词或动词中抽取原型成分，依然保证其合法性。
代码：lmm.py
4.词袋模型
the brown dog is running
the black dog is in the black room
runing in the room is forbidden
------------------------------------------------------------------
         the brown dog is running black in room forbddden
1       1    1           1     1  1             0        0  0        0
2       2    0           1     1  0             2        1   1       0
3       1    0           0     1  1             0        1   1       1
代码：bow.py
5.词频
单词在句子中出现的次数
--------------------------
      句子的总单词数
代码：tf.py
6.词频逆文档频率(TF-IDF)
                                                    总样本数
词频 x 逆文档频率 = 词频 x ---------------------
                                            包含该单词的样本数
代码：tfidf.py
文本分类，核心问题预测
xxxxxxxxxxxxxx -> 加解密
...
xxxxxxxxxxxxxx -> 摩托车
...
xxxxxxxxxxxxxx -> 棒球
------------------------------
xxxxxxxxxxxxxx -> ?
代码：doc.py
1 2 3 4 5 6
1 2 0 0 1 1
2 1 1 1 0 0
...
性别识别
feature
do   -> male
my  -> female
am  -> ...
om
ne
or
ne
代码：gndr.py
情感分析
xxx    xxx    xxx   ... xxx
True  False False ... True -> POSITIVE
代码：sent.py
主题词抽取
import gensim.models.ldamodel
LDA, Latent Dirichlet Allocation
隐含狄利克雷分布
代码：topic.py
十一、音频识别
1.模拟音频和数字音频
声带->机械振动->频率+响度=>声场强度=f(时间)
                                                                |
                                                               v
      耳朵<-播放器件机械振动<-电压/电流=f(时间)
                                                                | A/D
                                                                v
  .wav文件<-存储<-数字音频<-量化<-采样
                                       |
                                       v
                                    传输
                                       |
                                       v
                                 回放软件
                                       | D/A
                                       v
                         电压/电流=f(时间)->播放器件机械振动->耳朵
2.借助傅里叶变换提起频率特征
代码：sig.py
3.在频率特征的基础上结合语音的特点选择主要成分――MFCC，梅尔频率倒谱系数
                 关键频率1 关键频率2 关键频率3 ...
时域区间1        30           40            20        ... -> apple
时域区间2        10           20            50        ... -> apple
时域区间3        40           30            60        ... -> apple
...
代码：mfcc.py
4.语音识别
HMM: 隐马尔科夫模型
音频样本->MFCC->HMM->标签
代码：spch.py
语音->数字音频流->MFCC->学习模型->文本->TFIDF->模型
|<----------------语音识别--------------- >|                         |
                                                                    |                         v
                                                                    |                      语义
                                                                    |<自然语言处理>|
                                                                                              | 
                                                                                语音<-应答
                                                                                  语言合成
十二、图像识别
1.机器视觉工具包：OpenCV-Python
代码：basic.py
2.边缘检测
代码：edge.py
3.通过均衡直方提升亮度
代码：eq.py
4.角点检测
代码：corner.py
5.star特征检测
代码：star.py
6.sift特征检测
代码：sift.py
7.特征(描述)矩阵
代码：desc.py
8.图像识别
代码：obj.py
十三、人脸识别
1.视频捕捉
代码：vidcap.py
2.人脸定位
基于哈尔级联分类器的人脸定位
代码：haar.py
3.人脸识别
基于OpenCV的局部二值模式直方图(LBPH)模型
代码：face.py
十四、成分分析(CA)
1.主成分分析(PCA)
代码：np.py























