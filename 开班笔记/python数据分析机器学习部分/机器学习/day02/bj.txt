机器学习
一、人工智能、机器学习与深度学习
人工智能
       机器学习
              经典机器学习
              基于神经网络的机器学习
                     浅层学习
                     深层学习(深度学习)
              强化学习
              迁移学习
2->   ->4
3->   ->6
2.1->   ->4.2
1.9->   -> ? 3.8
二、机器学习基本类型
1.有监督学习：根据已知的输入和输出，建立联系它们的模型，根据该模型对未知输出的输入进行判断。
1)回归：以无限连续域的形式表示输出。
2)分类：以有限离散域的形式表示输出。
2.无监督学习：在一组没有已知输出(标签)的输入中，根据数据的内部特征和联系，找到某种规则，进行族群的划分――聚类。
3.半监督学习：从一个相对有限的已知结构中利用有监督学习的方法，构建基本模型，通过对未知输入和已知输入的比对，判断其输出，扩展原有的已知领域。
三、机器学习的基本过程
数据采集->数据清洗->数据预处理->选择模型->训练模型
原材料        去除杂质    准备               算法           规则
                                                                             |
                                                                             v
                                                           使用模型<-测试模型
                                                           业务生产    检验
四、数据预处理
              一列一特征
                        |
                        v
一行一样本 -> x x x x x \                  y y y
                        x x x x x  | 样本矩阵  y y y
                        x x x x x /                  y y y
姓名    年龄    身高    体重    ...
张飞    22       1.75   60      
赵云    20       1.80   70
...
1.均值移除
为了统一样本矩阵中不同特征的基准值和分散度，可以将各个特征的平均值调整为0，标准差调整为1，这个过程称为均值移除。
a b c
m=(a+b+c)/3
a-m b-m c-m
m'=(a-m+b-m+c-m)/3=(a+b+c)/3-3m/3=0
A B C
s=sqrt((A^2+B^2+C^2)/3)
A/s B/s C/s
s'=sqrt((A^2/s^2+B^2/s^2+C^2/s^2)/3)
   =sqrt((A^2+B^2+C^2)/s^2/3)
   =sqrt(s^2/s^2)
   =1
sklearn.preprocessing.scale(原始样本矩阵)
    ->均值移除后的样本矩阵
代码：std.py
2.范围缩放
统一样本矩阵中不同特征的最大值和最小值范围。
k x + b = y
k min + b = min'
k max + b = max'
sklearn.preprocessing.MinMaxScaler(
     feature_range=期望最小和最大值)->范围缩放器
范围缩放器.fit_transform(原始样本矩阵)
    ->范围缩放后的样本矩阵
代码：mms.py
3.归一化：为了用占比表示特征，用每个样本的特征值除以该样本的特征值绝对值之和，以使每个样本的特征值绝对值之和为1
          Python Java C/C++ PHP
2016 30         50     40        20    30/140
2017 20         30     20        10    20/80
sklearn.preprocessing.normalize(原始样本矩阵,
   norm='l1')->归一化后的样本矩阵
l1即L1范数，矢量中各元素绝对值之和。
代码：nor.py
4.二值化：用0和1来表示样本矩阵中相对于某个给定阈值高于或者低于它的元素。
sklearn.preprocessing.Binarizer(threshold=阈值)
    ->二值化器
二值化器.transform(原始样本矩阵)->二值化后的样本矩阵
代码：bin.py
5.独热编码
1        3        2
7        5        4
1        8        6
7        3        9
1:10  3:100 2:1000
7:01  5:010 4: 0100
          8:001 6: 0010
                     9: 0001
1 0 1 0 0 1 0 0 0
0 1 0 1 0 0 1 0 0
1 0 0 0 1 0 0 1 0
0 1 1 0 0 0 0 0 1
sklearn.preprocessing.OneHotEncoder(
    sparse=是否使用压缩格式, dtype=元素类型)
    ->独热编码器
独热编码器.fit_transform(原始样本矩阵)
    ->独热编码后的样本矩阵，同时构建编码表字典
独热编码器.transform(原始样本矩阵)
    ->独热编码后的样本矩阵，使用已有编码表字典
代码：ohe.py
6.标签编码：将字符形式的特征值映射为整数。
sklearn.preprocessing.LabelEncoder()->标签编码器
标签编码器.fit_transform(原始样本矩阵)
    ->编码样本矩阵，构建编码字典
标签编码器.transform(原始样本矩阵)
    ->编码样本矩阵，使用编码字典
标签编码器.inverse_transform(编码样本矩)
    ->原始样本矩阵，使用编码字典
代码：lab.py
五、线性回归
m个输入样本 -> m个输出标签
                 x1 -> y1
                 x2 -> y2
                 x3 -> y3
                 ...
                 xm -> ym
           xk + b -> y
1.预测函数：联系输出和输入的数学函数。
y=kx+b
其中的k和b称为模型参数，根据已知输入样本和对应的输出标签来训练得出。
2.均方误差：每一个已知输入样本所对应的实际输出标签和由模型预测出来的输出标签之间的误差平方的平均值。
kx1+b=y1'
kx2+b=y2'
kx3+b=y3'
...
kxm+b=ym'
(y1-y1')^2+(y2-y2')^2+(y3-y3')^2+...+(ym-ym')^2
-------------------------------------------------------------
                                         m
3.成本函数：将均方误差看作是关于模型参数的函数，谓之成本函数，记做J(k,b)。
线性回归问题的本质就是寻找能够使成本函数J(k,b)极小值的模型参数。
4.梯度下降
loss = J(k, b)
5.接口
sklearn.linear_model.LinearRegression()->线性回归器
线性回归器.fit(输入样本, 输出标签)
线性回归器.predict(输入样本)->预测输出标签
6.复用
通过pickle将内存中的模型对象写入磁盘文件，或从磁盘文件载入内存，以此保存训练好的模型，以备复用。
代码：line.py、save.py、load.py
六、岭回归
loss = J(k, b)+正则函数(样本权重)x正则强度
                                                          惩罚系数
sklearn.linear_model.Ridge(正则强度,
    fit_intercept=是否修正截距,
    max_iter=最大迭代次数)->岭回归器
代码：rdg.py
七、欠拟合与过拟合
欠拟合：无论是训练数据还是测试数据，模型给出的预测值和真实值都存在较大的误差。
过拟合：模型对于训练数据具有较高的精度，但对测试数据则表现极差。模型过于特殊，不够一般（泛化）。
欠拟合<--模型复杂度-->过拟合
八、多项式回归
x -> y                  y = kx + b
x x^2 -> y          y = k1x^2 + k2x + b
x x^2 x^3 -> y  y = k1x^3 + k2x^2 + k3x + b
sklearn.preprocessing.PolynomialFeatures(最高次数)
->多项式特征扩展器
sklearn.pipeline.make_pipe(多项式特征扩展器,
    线性回归器)->多项式回归器
 x-->多项式特征扩展器--x,x^2,x^3...-->线性回归器
                                                                        -->k1,k2,k3-->
代码：poly.py
九、决策树
相似的输入会有相似的输出。
0 - 专科    0-普通     0-女   0-差               0-低
1 - 本科    1-985     1-男   1-及格            1-中
2 - 硕士    2-211               2-良好            2-高
3 - 博士                             3-优异
      学历       院校     性别      成绩      ->      月薪
         1            0         1           2                  8000
         0            0         0           2                  7000
         3            1         1           3                  20000
         ...
         1            1         0           1                  ?
回归-平均
分类-投票
优化：
1)结合业务优先选择有限的主要特征，划分子表，降低决策树的高度；
2)根据香农定理计算根据每一个特征划分子表前后的信息熵差，选择熵减少量最大的特征，优先参与子表划分；
3)集合算法：根据不同方法，构建多个决策树，利用它们的预测结果，按照取均（回归）或投票（分类）的方法，产生最终的预测结果。
A.自助聚合：采用有放回抽样的规则，从m个样本中随机抽取n个样本，构建一棵决策树，重复以上过程b次，得到b棵决策树。利用每棵决策树的预测结果，根据平均或者投票得到最终预测结果。
B.随机森林：在自助聚合算法的基础上更进一步，对特征也应用自助聚合，即每次训练时，不是用所有的特征构建树结构，而是随机选择部分特征参与构建，以此避免特殊特征对预测结果的影响。
C.正向激励：初始化时，针对m个样本分配初始权重，然后根据这个带有权重的模型预测训练样本，针对那些预测错误的样本，提高其权重，再构建一棵决策树模型，重复以上过程，得到b棵决策树。。。
sklearn.tree.DecisionTreeRegressor()->决策树回归器
sklearn.ensemble.AdaBoostRegressor(元回归器,
    n_estimators=评估器数, radom_state=随机种子源)->正向激励回归器
sklearn.ensemble.RandomForestRegressor(
    max_depth=最大树高, n_estimators=评估器数,
    min_samples_split=划分子表的最小样本数)->随机森林回归器
代码：house.py
决策树模型.feature_importances_: 特征重要性
代码：fi.py、bike.py
三、简单分类器
输入    输出
3   1      0
2   5      1
1   8      1
6   4      0
5   2      0
3   5      1
4   7      1
4  -1     0
7   5      ?->0
代码：simple.py
四、逻辑分类
1.预测函数
x1 x2 -> y
              1
y = -----------
       1 + e^-z
z = k1x1 + k2x2 + b
2.成本函数
交叉熵误差
J(k1,k2,b) = sigma(-ylog(y')-(1-y)log(1-y'))/m +
                      m
                      正则函数(||k1,k2,b||)x正则强度
x x -> 0.9 1
x x -> 0.2 0
sklearn.linear_model.LogisticRegression(
    solver='liblinear', C=正则强度)
                  A   B    C
... -> A 1 0.9 0.1 0.3 A
... -> B 0 0.3 0.6 0.4 B
... -> C 0 0.1 0.2 0.6 C
代码：mlog.c
五、朴素贝叶斯分类
x x ... x  -> 0
x x ... x  -> 1
x x ... x  -> 0
x x ... x  -> 0
x x ... x  -> 1
x x ... x  -> 2
x x ... x  -> 1
x x ... x  -> 0
x x ... x  -> 2
...
1 9 ... x  -> 0 0.8
              -> 1 0.9 *
              -> 2 0.7
1.贝叶斯定理
                P(A)P(B|A)
P(A|B) = -------------
                     P(B)
2.朴素贝叶斯分类
求X样本属于C类别的概率，即当观察到X样本出现时，其所属的类别为C的概率：
P(C|X)=P(C)P(X|C)/P(X)
P(C)P(X|C)=P(C,X)=P(C,x1,x2,...,xn)
                                =P(x1,x2,...,xn,C)
                                =P(x1|x2,...,xn,C)P(x2,x3,...,xn,C)
=P(x1|x2,...,xn,C)P(x2|x3,...,xn,C)P(x3,x4,...,xn,C)
=P(x1|x2,...,xn,C)P(x2|x3,...,xn,C)P(x3|x4,...,xn,C)...P(C)
朴素：条件独立假设，即样本各个特征之间并无关联，不构成条件约束。
=P(x1|C)P(x2|C)P(x3|C)...P(C)
X样本属于C类别的概率，正比于C类别出现的的概率乘以C类别条件下X样本中每一个特征值出现的概率之乘积。



































